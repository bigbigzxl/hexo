
---
title: 梯度消失的原因
categories: ML  # 配置
---


[转载自哈工大SCIR（公众号）]()

为了弄清楚为何会出现消失的梯度，来看看一个极简单的深度神经网络：每一层都只有一个单一的神经元。下图就是有三层隐藏层的神经网络：
![](http://upload-images.jianshu.io/upload_images/4749583-b5244660c8be2a8a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-816294679aa5897b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-531b06189a5fa3bd?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-74d33b7863069d17?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-849b4ba1402e41c5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-c33bdee594a49e68?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-cbd53ee82851783e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](http://upload-images.jianshu.io/upload_images/4749583-76beb71017149b13?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>也就是说具体是梯度消失还是梯度爆炸，实质上就是w参数来决定的（在选定sigmod的情况下）。

>还有就是我们求梯度的时候是用代价函数来对参数求导的，由于神经元一级一级地传下来的时候，本质上就是一个复合函数了，因此求导的话必须采用复合函数求导法则，也就是每个神经元里面的sigmod都会被求导一次，而且连续相乘，由于我们知道sigmod函数的最大值是1/4，因此w参数乘以sigmod导数大于1还是小于1就一个判断梯度消失还是爆炸的标准了。

>**总结：**就是处于某种场景模式下，出现了多元变量的连乘情况，这个情况一化简（连乘的每项（w*σ｀）均是否大于1），就是一个中学数学题了！所以啊！很多复杂的概念其实都是表面复杂，本质上的脊梁骨还是很简单初级的！