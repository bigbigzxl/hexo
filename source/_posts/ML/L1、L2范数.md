---
title: L1/L2范数
categories: ML  # 配置
---

文章：[这个博客](http://t.hengwei.me/post/%E6%B5%85%E8%B0%88l0l1l2%E8%8C%83%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html) 里面介绍了L1、L2范数，说到了**L1正则化产生稀疏的权值, L2正则化产生平滑的权值**，何为如此？


[这个博客讲的通俗易懂啊！](https://vimsky.com/article/969.html)因此我也顺着思路学习了下！

我们知道机器学习实践过程中会经常使用L1、L2范数来进行正则化，目的是限制权值的大小，从而减少过拟合的分线。

在前面有一节是讲到了梯度下降法，这一节就结合正则来一起捋一捋它是如何减少目标函数过拟合的！`（因为我们知道权值过多就类似谐波过多，能很精细地拟合函数，但是过犹不及，你拟合了群体里面的一个个体，那是对其他人的不公，比如社会这个大群体，里面有人民这个群体，你去用个模型拟合，谁知你却用了很多的权重去拟合王健林，这下好了，出来描述整个人民群体的模型跟实际情况出入很大，引起民愤啊！）`

比较喜欢从**数学的角度**来理解问题：

目标函数梯度下降就是修改权值的嘛！我们专注问题的本质，就看权值：

![](http://upload-images.jianshu.io/upload_images/4749583-2627b5de5f43cf3b.gif?imageMogr2/auto-orient/strip)

为了进行一般性讨论我们取Wi为非0的某个正浮点数，学习率η=0.5，那么：

#### L1正则化产生稀疏的权值
L1的权值更新公式为**Wi=Wi - η\*1**（因为|Wi|的偏导不就是1么！还是+1，因为绝对值啊！），这个式子表达的是每走一步都减少一个特定的值（因为学习率是定了的嘛！），因此最终肯定会减少到0的啊！**减少到0的话，对应的特征就GAME OVER了，因此稀疏性就体现出来了！**

![](http://upload-images.jianshu.io/upload_images/4749583-e39c2e17ea39ff16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### L2正则化产生平滑的权值
L2的权值更新公式为**Wi = Wi - η\*Wi = Wi - 0.5\*Wi**，也就是说每迭代一次权值就变成上一次的一半了，虽然权值在不断减少，但是却永远也不会到零啊，因为越减到后面你的递减量也变小了啊！


![2222.png](http://upload-images.jianshu.io/upload_images/4749583-223eaf796f1c637c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

